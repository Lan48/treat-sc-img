# make sample with size of 14*14*200
import argparse
import os
import json
import h5py
import numpy as np
from scipy import sparse
from tqdm import tqdm
import anndata as ad
from scipy import stats
import random
from functools import lru_cache
from typing import List, Tuple, Dict, Any
from scipy.sparse import csr_matrix 

# find high-variance genes and save
def find_hv_genes(input_dir, depth,hv_genes_path):
    """ä¼˜åŒ–åçš„é«˜å˜åŸºå› é€‰æ‹©å‡½æ•°ï¼Œæ ¸å¿ƒæ”¹è¿›ï¼š
    1. ä½¿ç”¨å†…å­˜æ˜ å°„ä»£æ›¿å…¨é‡åŠ è½½çŸ©é˜µ
    2. å¢é‡ç»Ÿè®¡æ›¿ä»£å…¨æ•°æ®å­˜å‚¨
    3. åˆ†å—å¤„ç†å‡å°‘å†…å­˜å³°å€¼
    4. å‘é‡åŒ–è®¡ç®—æå‡æ•ˆç‡
    """
    print('Finding high-variance genes...')
    # é˜¶æ®µ1: å…¨å±€åŸºå› IDæ”¶é›† (ä½å†…å­˜æ¶ˆè€—)
    print('1: Collecting all gene IDs...')
    all_genes = set()
    for fname in os.listdir(input_dir):
        if not fname.endswith('.h5'):
            continue
        with h5py.File(os.path.join(input_dir, fname), 'r') as f:
            for group in f.values():
                if 'vocab_index' not in group:
                    continue
                # é«˜æ•ˆå¤„ç†å­—èŠ‚å­—ç¬¦ä¸²è½¬æ¢
                vocab = group['vocab_index'][:]
                all_genes.update(v.decode() if isinstance(v, bytes) else str(v) for v in vocab)

    # é˜¶æ®µ2: å¢é‡å¼ç»Ÿè®¡è®¡ç®—
    print('2: Computing gene statistics...')
    gene_stats = {gene: [0, 0.0, 0.0] for gene in all_genes}  # [count, sum, sum_sq]
    chunk_size = 2000  # ä¼˜åŒ–å†…å­˜çš„å…³é”®å‚æ•°
    
    for fname in os.listdir(input_dir):
        if not fname.endswith('.h5'):
            continue
            
        with h5py.File(os.path.join(input_dir, fname), 'r') as f:
            for group in f.values():
                # è·³è¿‡æ— æ•ˆæ•°æ®é›†
                if 'expr_map' not in group or 'vocab_index' not in group:
                    continue
                    
                ds = group['expr_map']
                vocab = group['vocab_index'][:]
                gene_ids = [v.decode() if isinstance(v, bytes) else str(v) for v in vocab]
                
                n_genes = ds.shape[1]
                
                # åˆ†å—å¤„ç†å¤§æ•°æ®é›†
                for start in range(0, n_genes, chunk_size):
                    end = min(start + chunk_size, n_genes)
                    # ä½¿ç”¨memory-mapé¿å…å…¨é‡åŠ è½½
                    chunk = ds[:, start:end]
                    
                    # å‘é‡åŒ–è®¡ç®—ç»Ÿè®¡é‡
                    sums = chunk.sum(axis=0)
                    sum_sqs = (chunk**2).sum(axis=0)
                    counts = np.full(chunk.shape[1], chunk.shape[0])
                    
                    # å¢é‡æ›´æ–°ç»Ÿè®¡å€¼
                    for i in range(chunk.shape[1]):
                        gene = gene_ids[start + i]
                        cnt, s, sq = gene_stats[gene]
                        cnt_new = cnt + counts[i]
                        s_new = s + sums[i]
                        sq_new = sq + sum_sqs[i]
                        gene_stats[gene] = [cnt_new, s_new, sq_new]

    # é˜¶æ®µ3: é«˜æ•ˆè®¡ç®—å˜å¼‚ç³»æ•°
    print('3: Computing gene CVs...')
    epsilon = 1e-7  # é˜²æ­¢é™¤é›¶
    cv_values = []
    
    for gene, (n, s, sq) in gene_stats.items():
        if n == 0:
            cv = 0.0
        else:
            mean = s / n
            # ä½¿ç”¨æ•°å€¼ç¨³å®šå…¬å¼è®¡ç®—æ–¹å·®
            variance = max(0.0, (sq - s**2 / n) / (n - 1)) if n > 1 else 0.0
            cv = np.sqrt(variance) / (mean + epsilon)
        cv_values.append((gene, cv))
    
    # é˜¶æ®µ4: åŸºäºå˜å¼‚ç³»æ•°æ’åºå–TopK
    print(f'4: Top {depth} high-variance genes:')
    cv_values.sort(key=lambda x: x[1], reverse=True)
    hv_genes_id = [gene for gene, _ in cv_values[:depth]]
    # é˜¶æ®µ5: ä¿å­˜Top K åŸºå› ID
    print('5: Saving Top K gene IDs...')
    output_path = hv_genes_path
    with open(output_path, 'w') as f:
        json.dump(hv_genes_id, f, indent=4)
    return hv_genes_id

# make h5 file into samples
'''
def make_samples(input_dir, output_dir, seed=0, height=14, width=14, depth=200,min_spot=10, pad_id=0, n_samples_multiplier=2):
    """
    å¤„ç†ç©ºé—´è½¬å½•ç»„æ•°æ®ï¼Œåˆ›å»ºéšæœºé‡‡æ ·çš„ç©ºé—´åŒºåŸŸæ ·æœ¬ï¼Œä¿å­˜ä¸ºH5ADæ ¼å¼
    
    ä¿®æ”¹äº®ç‚¹:
    1. æ·»åŠ pad_idå‚æ•°: å½“æ ·æœ¬åŸºå› æ•°é‡ä¸è¶³depthæ—¶ï¼Œç”¨pad_idå¡«å……
    2. æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹è®¡ç®—é«˜å˜åŸºå› ï¼Œç„¶ååŠ æƒéšæœºé€‰æ‹©depthä¸ªåŸºå› 
    3. éšæœºé‡‡æ ·ç©ºé—´åŒºåŸŸè€Œéæ•´ä½“åˆ†å‰²
    4. å¢åŠ é‡‡æ ·æ¬¡æ•°(åŸæœ¬åŒºå—æ•°*2)
    
    å‚æ•°:
    input_dir: åŒ…å«.h5è¾“å…¥æ–‡ä»¶çš„ç›®å½•
    output_dir: è¾“å‡ºæ ·æœ¬çš„ç›®å½•
    seed: éšæœºç§å­(é»˜è®¤0)
    height: æ¯ä¸ªæ ·æœ¬çš„yè½´é«˜åº¦(é»˜è®¤14)
    width: æ¯ä¸ªæ ·æœ¬çš„xè½´å®½åº¦(é»˜è®¤14)
    depth: ä½¿ç”¨çš„åŸºå› æ•°é‡(é»˜è®¤200)
    min_spot: æ ·æœ¬åŒ…å«çš„æœ€å°spotæ•°(é»˜è®¤10)
    pad_id: ç”¨äºå¡«å……ä¸è¶³åŸºå› çš„æ ‡è¯†ç¬¦(é»˜è®¤"PAD")
    n_samples_multiplier: é‡‡æ ·æ¬¡æ•°å€æ•°(é»˜è®¤2)
    
    # Spotçº§åˆ«å…ƒæ•°æ®
    adata.obs = {
        'original_index': åŸå§‹ç´¢å¼•,
        'x_abs': ç»å¯¹Xåæ ‡,
        'y_abs': ç»å¯¹Yåæ ‡,
        'x_rel': æ ·æœ¬å†…ç›¸å¯¹Xåæ ‡,
        'y_rel': æ ·æœ¬å†…ç›¸å¯¹Yåæ ‡
    }

    # åŸºå› çº§åˆ«å…ƒæ•°æ®
    adata.var = {
        'gene_ids': åŸºå› IDåˆ—è¡¨(å«pad_idå¡«å……),
        'is_selected': åŸºå› æ˜¯å¦è¢«é€‰æ‹©çš„å¸ƒå°”å‘é‡
    }

    # æ ·æœ¬å…ƒæ•°æ®
    adata.uns['region'] = {
        'x_start': åŒºåŸŸèµ·å§‹X,
        'y_start': åŒºåŸŸèµ·å§‹Y,
        'width': åŒºåŸŸå®½åº¦,
        'height': åŒºåŸŸé«˜åº¦
    }
    """
    # è®¾ç½®éšæœºç§å­
    np.random.seed(seed)
    random.seed(seed)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰h5æ–‡ä»¶
    h5_files = [f for f in os.listdir(input_dir) if f.endswith('.h5')]
    if not h5_files:
        print("âš ï¸ è­¦å‘Š: è¾“å…¥ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°.h5æ–‡ä»¶")
        return
    
    print(f"ğŸ” æ‰¾åˆ° {len(h5_files)} ä¸ª.h5æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
    print(f"âš™ï¸ é…ç½®å‚æ•°: height={height}, width={width}, depth={depth}, min_spot={min_spot}")
    print(f"ğŸ² é‡‡æ ·ç­–ç•¥: éšæœºé‡‡æ ·{height}x{width}åŒºåŸŸ")
    print(f"ğŸ’¾ è¾“å‡ºæ ¼å¼: H5AD (AnnDataæ ¼å¼)")
    
    # ç»Ÿè®¡å˜é‡
    total_samples = 0
    skipped_files = 0
    skipped_groups = 0
    skipped_blocks = 0
    
    # è¿›åº¦æ¡ï¼šæ–‡ä»¶å¤„ç†
    file_pbar = tqdm(h5_files, desc="æ–‡ä»¶å¤„ç†")
    
    for file_name in file_pbar:
        file_pbar.set_postfix(file=file_name)
        file_path = os.path.join(input_dir, file_name)
        
        try:
            with h5py.File(file_path, 'r') as h5_file:
                groups = list(h5_file.keys())
                if not groups:
                    skipped_files += 1
                    print(f"âš ï¸ è·³è¿‡: æ–‡ä»¶ {file_name} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»„")
                    continue
                
                # ç»„å¤„ç†è¿›åº¦
                group_pbar = tqdm(groups, desc=f"æ ·æœ¬å¤„ç†", leave=False)
                
                for group_name in group_pbar:
                    group_pbar.set_postfix(group=group_name)
                    
                    try:
                        group = h5_file[group_name]
                        
                        # æå–æ•°æ®
                        coords = group['coords_map'][:]
                        expr = group['expr_map'][:]
                        vocab = group['vocab_index'][:]
                        
                        # å¤„ç†åŸºå› IDç±»å‹
                        if isinstance(vocab[0], bytes):
                            vocab = np.array([g.decode('utf-8') for g in vocab])
                        elif np.issubdtype(vocab.dtype, np.integer):
                            vocab = np.array([str(g) for g in vocab])
                        
                        n_spots, n_genes = expr.shape
                        
                        # è®¡ç®—æ¯ä¸ªåŸºå› çš„æ•´ä½“æ–¹å·®ï¼ˆç”¨äºæƒé‡ï¼‰
                        overall_var = np.var(expr, axis=0)
                        
                        # ç¡®å®šç©ºé—´èŒƒå›´
                        x_min, x_max = coords[:, 0].min(), coords[:, 0].max()
                        y_min, y_max = coords[:, 1].min(), coords[:, 1].max()
                        
                        # è®¡ç®—åŸæœ¬çš„åˆ†å—æ•°é‡ï¼ˆç”¨äºç¡®å®šé‡‡æ ·æ¬¡æ•°ï¼‰
                        x_blocks = int(np.ceil((x_max - x_min + 1) / width))
                        y_blocks = int(np.ceil((y_max - y_min + 1) / height))
                        n_samples = x_blocks * y_blocks * n_samples_multiplier
                        
                        sample_count = 1
                        spots_added = 0
                        
                        # åŒºå—å¤„ç†è¿›åº¦
                        block_pbar = tqdm(
                            total=n_samples, 
                            desc=f"éšæœºé‡‡æ ·", 
                            leave=False
                        )
                        
                        # éšæœºé‡‡æ ·ç©ºé—´åŒºå—
                        for _ in range(n_samples):
                            block_pbar.update(1)
                            
                            # éšæœºé€‰æ‹©èµ·å§‹ç‚¹
                            x_start = random.randint(int(x_min), max(int(x_min), int(x_max) - width))
                            y_start = random.randint(int(y_min), max(int(y_min), int(y_max) - height))
                            x_end = x_start + width
                            y_end = y_start + height
                            
                            # é€‰æ‹©åŒºå—å†…çš„spots
                            in_block = (coords[:, 0] >= x_start) & (coords[:, 0] < x_end) & \
                                       (coords[:, 1] >= y_start) & (coords[:, 1] < y_end)
                            
                            # è·³è¿‡spotä¸è¶³çš„åŒºå—
                            num_spots = np.sum(in_block)
                            if num_spots < min_spot:
                                skipped_blocks += 1
                                continue
                            
                            block_coords = coords[in_block]
                            block_expr = expr[in_block]
                            rel_coords = block_coords - np.array([x_start, y_start])
                            
                            # === é€‰æ‹©é«˜å˜åŸºå›  ===
                            
                            # è®¡ç®—åŒºåŸŸå†…æ¯ä¸ªåŸºå› çš„æ–¹å·®
                            block_var = np.var(block_expr, axis=0)
                            
                            # ç»“åˆå…¨å±€æ–¹å·®å’ŒåŒºåŸŸæ–¹å·®çš„æƒé‡
                            alpha = 1.0  # åŒºåŸŸæ–¹å·®æƒé‡
                            beta = 0   # å…¨å±€æ–¹å·®æƒé‡
                            gene_weights = alpha * block_var + beta * overall_var
                            
                            # æ ‡å‡†åŒ–æƒé‡å¹¶æ·»åŠ å°å¸¸æ•°é¿å…é›¶æƒé‡
                            gene_weights = (gene_weights - np.min(gene_weights)) / \
                                          (np.max(gene_weights) - np.min(gene_weights) + 1e-8) + 1e-8
                            
                            # åŠ æƒéšæœºé€‰æ‹©depthä¸ªåŸºå› ï¼ˆæ— é‡å¤ï¼‰
                            selected_indices = random.choices(
                                range(n_genes), 
                                weights=gene_weights, 
                                k=min(depth, n_genes)
                            )
                            
                            # å¦‚æœéœ€è¦ï¼Œä½¿ç”¨pad_idå¡«å……
                            selected_gene_ids = []
                            selected_expr = np.zeros((num_spots, depth))
                            
                            # å¤åˆ¶é€‰ä¸­çš„åŸºå› è¡¨è¾¾å€¼
                            for i, idx in enumerate(selected_indices):
                                selected_gene_ids.append(vocab[idx])
                                selected_expr[:, i] = block_expr[:, idx]
                            
                            # å¡«å……ä¸è¶³çš„åŸºå› 
                            if len(selected_indices) < depth:
                                fill_count = depth - len(selected_indices)
                                selected_gene_ids.extend([pad_id] * fill_count)
                            
                            # åˆ›å»ºæ˜¯å¦è¢«é€‰ä¸­çš„æ ‡è®°
                            is_selected = [True] * len(selected_indices) + [False] * (depth - len(selected_indices))
                            
                            # åˆ›å»ºAnnDataå¯¹è±¡
                            adata = ad.AnnData(
                                X=selected_expr,
                                obs={
                                    'original_index': np.where(in_block)[0],
                                    'x_abs': block_coords[:, 0],
                                    'y_abs': block_coords[:, 1],
                                    'x_rel': rel_coords[:, 0],
                                    'y_rel': rel_coords[:, 1]
                                },
                                var={
                                    'gene_ids': selected_gene_ids,
                                    'is_selected': is_selected
                                }
                            )
                            
                            # æ·»åŠ ç©ºé—´ä¿¡æ¯åˆ°obsm
                            adata.obsm['coords_map'] = block_coords
                            adata.obsm['coords_sample'] = rel_coords
                            
                            # æ·»åŠ é¢å¤–å…ƒæ•°æ®
                            adata.uns['region'] = {
                                'x_start': x_start,
                                'y_start': y_start,
                                'width': width,
                                'height': height
                            }
                            
                            adata.uns['gene_selection'] = {
                                'method': 'weighted_random',
                                'alpha': alpha,
                                'beta': beta
                            }
                            
                            # ä¿å­˜ä¸ºH5AD
                            output_name = f"{group_name}_sample_{sample_count}.h5ad"
                            output_path = os.path.join(output_dir, output_name)
                            adata.write(output_path)
                            
                            sample_count += 1
                            total_samples += 1
                            spots_added += num_spots
                        
                        block_pbar.close()
                        group_pbar.set_postfix(spots=f"{spots_added} spots")
                    
                    except Exception as e:
                        skipped_groups += 1
                        print(f"âš ï¸ è·³è¿‡ç»„ {group_name}: {str(e)}")
                
                group_pbar.close()
        
        except Exception as e:
            skipped_files += 1
            print(f"ğŸš¨ é”™è¯¯å¤„ç†æ–‡ä»¶ {file_name}: {str(e)}")
    
    # å¤„ç†å®Œæˆåçš„ç»Ÿè®¡ä¿¡æ¯
    print(f"\nâœ… å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"  åˆ›å»ºæ ·æœ¬: {total_samples}ä¸ª")
    print(f"  é‡‡æ ·å°è¯•: {x_blocks * y_blocks * n_samples_multiplier}æ¬¡")
    print(f"  è·³è¿‡æ–‡ä»¶: {skipped_files}ä¸ª")
    print(f"  è·³è¿‡æ ·æœ¬ç»„: {skipped_groups}ä¸ª")
    print(f"  è·³è¿‡åŒºå—: {skipped_blocks}ä¸ª (spot<{min_spot})")
    
    if total_samples == 0:
        print("âŒ æœªåˆ›å»ºä»»ä½•æ ·æœ¬ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®å’Œå‚æ•°é…ç½®")
    else:
        print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {output_dir} (H5ADæ ¼å¼)")
        print(f"ğŸ”¬ æ¯ä¸ªæ ·æœ¬åŒ…å« {depth} ä¸ªåŸºå›  (ä½¿ç”¨ {pad_id} å¡«å……ä¸è¶³)")

def make_samples_h5(input_dir, output_dir, seed=0, height=14, width=14, depth=200,
                 min_spot=10, pad_id=0, n_samples_multiplier=2):
    """
    å¤„ç†ç©ºé—´è½¬å½•ç»„æ•°æ®ï¼Œä»¥ç»†èƒåæ ‡ä¸ºä¸­å¿ƒåˆ›å»ºç©ºé—´åŒºåŸŸæ ·æœ¬ï¼Œä¿å­˜ä¸ºH5ADæ ¼å¼
    
    ä¿®æ”¹äº®ç‚¹:
    1. æ·»åŠ pad_idå‚æ•°: å½“æ ·æœ¬åŸºå› æ•°é‡ä¸è¶³depthæ—¶ï¼Œç”¨pad_idå¡«å……
    2. æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹è®¡ç®—é«˜å˜åŸºå› ï¼Œç„¶ååŠ æƒéšæœºé€‰æ‹©depthä¸ªåŸºå› 
    3. ä»¥ç»†èƒåæ ‡ä¸ºä¸­å¿ƒé‡‡æ ·ç©ºé—´åŒºåŸŸè€ŒééšæœºåŒºåŸŸ
    4. é‡‡æ ·æ¬¡æ•°ä¸ºç»†èƒæ•°é‡çš„ä¸¤å€
    
    å‚æ•°:
    input_dir: åŒ…å«.h5è¾“å…¥æ–‡ä»¶çš„ç›®å½•
    output_dir: è¾“å‡ºæ ·æœ¬çš„ç›®å½•
    seed: éšæœºç§å­(é»˜è®¤0)
    height: æ¯ä¸ªæ ·æœ¬çš„yè½´é«˜åº¦(é»˜è®¤14)
    width: æ¯ä¸ªæ ·æœ¬çš„xè½´å®½åº¦(é»˜è®¤14)
    depth: ä½¿ç”¨çš„åŸºå› æ•°é‡(é»˜è®¤200)
    min_spot: æ ·æœ¬åŒ…å«çš„æœ€å°spotæ•°(é»˜è®¤10)
    pad_id: ç”¨äºå¡«å……ä¸è¶³åŸºå› çš„æ ‡è¯†ç¬¦(é»˜è®¤"PAD")
    n_samples_multiplier: é‡‡æ ·æ¬¡æ•°å€æ•°(é»˜è®¤2)
    
    # Spotçº§åˆ«å…ƒæ•°æ®
    adata.obs = {
        'original_index': åŸå§‹ç´¢å¼•,
        'x_abs': ç»å¯¹Xåæ ‡,
        'y_abs': ç»å¯¹Yåæ ‡,
        'x_rel': æ ·æœ¬å†…ç›¸å¯¹Xåæ ‡,
        'y_rel': æ ·æœ¬å†…ç›¸å¯¹Yåæ ‡
    }

    # åŸºå› çº§åˆ«å…ƒæ•°æ®
    adata.var = {
        'gene_ids': åŸºå› IDåˆ—è¡¨(å«pad_idå¡«å……),
        'is_selected': åŸºå› æ˜¯å¦è¢«é€‰æ‹©çš„å¸ƒå°”å‘é‡
    }

    # æ ·æœ¬å…ƒæ•°æ®
    adata.uns['region'] = {
        'x_start': åŒºåŸŸèµ·å§‹X,
        'y_start': åŒºåŸŸèµ·å§‹Y,
        'width': åŒºåŸŸå®½åº¦,
        'height': åŒºåŸŸé«˜åº¦
    }
    """
    # è®¾ç½®éšæœºç§å­
    np.random.seed(seed)
    random.seed(seed)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰h5æ–‡ä»¶
    h5_files = [f for f in os.listdir(input_dir) if f.endswith('.h5')]
    if not h5_files:
        print("âš ï¸ è­¦å‘Š: è¾“å…¥ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°.h5æ–‡ä»¶")
        return
    
    print(f"ğŸ” æ‰¾åˆ° {len(h5_files)} ä¸ª.h5æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
    print(f"âš™ï¸ é…ç½®å‚æ•°: height={height}, width={width}, depth={depth}, min_spot={min_spot}")
    print(f"ğŸ¯ é‡‡æ ·ç­–ç•¥: ä»¥ç»†èƒåæ ‡ä¸ºä¸­å¿ƒé‡‡æ ·{height}x{width}åŒºåŸŸ")
    print(f"ğŸ’¾ è¾“å‡ºæ ¼å¼: H5AD (AnnDataæ ¼å¼)")
    
    # ç»Ÿè®¡å˜é‡
    total_samples = 0
    skipped_files = 0
    skipped_groups = 0
    skipped_blocks = 0
    
    # è¿›åº¦æ¡ï¼šæ–‡ä»¶å¤„ç†
    file_pbar = tqdm(h5_files, desc="æ–‡ä»¶å¤„ç†")
    
    for file_name in file_pbar:
        file_pbar.set_postfix(file=file_name)
        file_path = os.path.join(input_dir, file_name)
        
        try:
            with h5py.File(file_path, 'r') as h5_file:
                groups = list(h5_file.keys())
                if not groups:
                    skipped_files += 1
                    print(f"âš ï¸ è·³è¿‡: æ–‡ä»¶ {file_name} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»„")
                    continue
                
                # ç»„å¤„ç†è¿›åº¦
                group_pbar = tqdm(groups, desc=f"æ ·æœ¬å¤„ç†", leave=False)
                
                for group_name in group_pbar:
                    group_pbar.set_postfix(group=group_name)
                    
                    try:
                        group = h5_file[group_name]
                        
                        # æå–æ•°æ®
                        coords = group['coords_map'][:]
                        expr = group['expr_map'][:]
                        vocab = group['vocab_index'][:]
                        
                        # å¤„ç†åŸºå› IDç±»å‹
                        if isinstance(vocab[0], bytes):
                            vocab = np.array([g.decode('utf-8') for g in vocab])
                        elif np.issubdtype(vocab.dtype, np.integer):
                            vocab = np.array([str(g) for g in vocab])
                        
                        n_spots, n_genes = expr.shape
                        
                        # è®¡ç®—æ¯ä¸ªåŸºå› çš„æ•´ä½“æ–¹å·®ï¼ˆç”¨äºæƒé‡ï¼‰
                        overall_var = np.var(expr, axis=0)
                        
                        # ç¡®å®šç©ºé—´èŒƒå›´
                        x_min, x_max = coords[:, 0].min(), coords[:, 0].max()
                        y_min, y_max = coords[:, 1].min(), coords[:, 1].max()
                        
                        # è®¡ç®—é‡‡æ ·æ¬¡æ•°ï¼šç»†èƒæ•°é‡çš„ä¸¤å€
                        n_samples = n_spots * n_samples_multiplier
                        
                        sample_count = 1
                        spots_added = 0
                        
                        # åŒºå—å¤„ç†è¿›åº¦
                        block_pbar = tqdm(
                            total=n_samples, 
                            desc=f"ç»†èƒä¸­å¿ƒé‡‡æ ·", 
                            leave=False
                        )
                        
                        # ä»¥ç»†èƒåæ ‡ä¸ºä¸­å¿ƒè¿›è¡Œé‡‡æ ·
                        for _ in range(n_samples):
                            block_pbar.update(1)
                            
                            # éšæœºé€‰æ‹©ä¸€ä¸ªç»†èƒä½œä¸ºä¸­å¿ƒç‚¹
                            center_idx = random.randint(0, n_spots - 1)
                            center_x, center_y = coords[center_idx]
                            
                            # è®¡ç®—é‡‡æ ·åŒºåŸŸçš„èµ·å§‹åæ ‡ï¼ˆç¡®ä¿åŒºåŸŸåœ¨åˆ‡ç‰‡èŒƒå›´å†…ï¼‰
                            x_start = max(x_min, center_x - width // 2)
                            y_start = max(y_min, center_y - height // 2)
                            
                            # è°ƒæ•´èµ·å§‹ç‚¹ï¼Œç¡®ä¿åŒºåŸŸä¸è¶…å‡ºè¾¹ç•Œ
                            x_start = min(x_start, x_max - width)
                            y_start = min(y_start, y_max - height)
                            
                            # ç¡®ä¿èµ·å§‹åæ ‡ä¸ºæ•´æ•°
                            x_start = int(x_start)
                            y_start = int(y_start)
                            
                            x_end = x_start + width
                            y_end = y_start + height
                            
                            # é€‰æ‹©åŒºå—å†…çš„spots
                            in_block = (coords[:, 0] >= x_start) & (coords[:, 0] < x_end) & \
                                       (coords[:, 1] >= y_start) & (coords[:, 1] < y_end)
                            
                            # è·³è¿‡spotä¸è¶³çš„åŒºå—
                            num_spots = np.sum(in_block)
                            if num_spots < min_spot:
                                skipped_blocks += 1
                                continue
                            
                            block_coords = coords[in_block]
                            block_expr = expr[in_block]
                            rel_coords = block_coords - np.array([x_start, y_start])
                            
                            # === é€‰æ‹©é«˜å˜åŸºå›  ===
                            
                            # è®¡ç®—åŒºåŸŸå†…æ¯ä¸ªåŸºå› çš„æ–¹å·®
                            block_var = np.var(block_expr, axis=0)
                            
                            # ç»“åˆå…¨å±€æ–¹å·®å’ŒåŒºåŸŸæ–¹å·®çš„æƒé‡
                            alpha = 1.0  # åŒºåŸŸæ–¹å·®æƒé‡
                            beta = 0   # å…¨å±€æ–¹å·®æƒé‡
                            gene_weights = alpha * block_var + beta * overall_var
                            
                            # æ ‡å‡†åŒ–æƒé‡å¹¶æ·»åŠ å°å¸¸æ•°é¿å…é›¶æƒé‡
                            gene_weights = (gene_weights - np.min(gene_weights)) / \
                                          (np.max(gene_weights) - np.min(gene_weights) + 1e-8) + 1e-8
                            
                            # åŠ æƒéšæœºé€‰æ‹©depthä¸ªåŸºå› ï¼ˆæ— é‡å¤ï¼‰
                            selected_indices = random.choices(
                                range(n_genes), 
                                weights=gene_weights, 
                                k=min(depth, n_genes)
                            )
                            
                            # å¦‚æœéœ€è¦ï¼Œä½¿ç”¨pad_idå¡«å……
                            selected_gene_ids = []
                            selected_expr = np.zeros((num_spots, depth))
                            
                            # å¤åˆ¶é€‰ä¸­çš„åŸºå› è¡¨è¾¾å€¼
                            for i, idx in enumerate(selected_indices):
                                selected_gene_ids.append(vocab[idx])
                                selected_expr[:, i] = block_expr[:, idx]
                            
                            # å¡«å……ä¸è¶³çš„åŸºå› 
                            if len(selected_indices) < depth:
                                fill_count = depth - len(selected_indices)
                                selected_gene_ids.extend([pad_id] * fill_count)
                            
                            # åˆ›å»ºæ˜¯å¦è¢«é€‰ä¸­çš„æ ‡è®°
                            is_selected = [True] * len(selected_indices) + [False] * (depth - len(selected_indices))
                            
                            # åˆ›å»ºAnnDataå¯¹è±¡
                            adata = ad.AnnData(
                                X=selected_expr,
                                obs={
                                    'original_index': np.where(in_block)[0],
                                    'x_abs': block_coords[:, 0],
                                    'y_abs': block_coords[:, 1],
                                    'x_rel': rel_coords[:, 0],
                                    'y_rel': rel_coords[:, 1]
                                },
                                var={
                                    'gene_ids': selected_gene_ids,
                                    'is_selected': is_selected
                                }
                            )
                            
                            # æ·»åŠ ç©ºé—´ä¿¡æ¯åˆ°obsm
                            adata.obsm['coords_map'] = block_coords
                            adata.obsm['coords_sample'] = rel_coords
                            
                            # æ·»åŠ é¢å¤–å…ƒæ•°æ®
                            adata.uns['region'] = {
                                'x_start': x_start,
                                'y_start': y_start,
                                'width': width,
                                'height': height
                            }
                            
                            adata.uns['gene_selection'] = {
                                'method': 'weighted_random',
                                'alpha': alpha,
                                'beta': beta
                            }
                            
                            # æ·»åŠ ä¸­å¿ƒç»†èƒä¿¡æ¯
                            adata.uns['center_cell'] = {
                                'x': center_x,
                                'y': center_y,
                                'index': center_idx
                            }
                            
                            # ä¿å­˜ä¸ºH5AD
                            output_name = f"{group_name}_sample_{sample_count}.h5ad"
                            output_path = os.path.join(output_dir, output_name)
                            adata.write(output_path)
                            
                            sample_count += 1
                            total_samples += 1
                            spots_added += num_spots
                        
                        block_pbar.close()
                        group_pbar.set_postfix(spots=f"{spots_added} spots")
                    
                    except Exception as e:
                        skipped_groups += 1
                        print(f"âš ï¸ è·³è¿‡ç»„ {group_name}: {str(e)}")
                
                group_pbar.close()
        
        except Exception as e:
            skipped_files += 1
            print(f"ğŸš¨ é”™è¯¯å¤„ç†æ–‡ä»¶ {file_name}: {str(e)}")
    
    # å¤„ç†å®Œæˆåçš„ç»Ÿè®¡ä¿¡æ¯
    print(f"\nâœ… å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"  åˆ›å»ºæ ·æœ¬: {total_samples}ä¸ª")
    print(f"  é‡‡æ ·å°è¯•: {n_samples}æ¬¡ (ç»†èƒæ•°é‡Ã—{n_samples_multiplier})")
    print(f"  è·³è¿‡æ–‡ä»¶: {skipped_files}ä¸ª")
    print(f"  è·³è¿‡æ ·æœ¬ç»„: {skipped_groups}ä¸ª")
    print(f"  è·³è¿‡åŒºå—: {skipped_blocks}ä¸ª (spot<{min_spot})")
    
    if total_samples == 0:
        print("âŒ æœªåˆ›å»ºä»»ä½•æ ·æœ¬ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®å’Œå‚æ•°é…ç½®")
    else:
        print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {output_dir} (H5ADæ ¼å¼)")
        print(f"ğŸ”¬ æ¯ä¸ªæ ·æœ¬åŒ…å« {depth} ä¸ªåŸºå›  (ä½¿ç”¨ {pad_id} å¡«å……ä¸è¶³)")
'''



def scale_coordinates(coords: np.ndarray, target_size: int = 560) -> np.ndarray:
    """
    å°†åæ ‡ç¼©æ”¾è‡³target_size x target_sizeèŒƒå›´
    
    å‚æ•°:
    coords: åŸå§‹åæ ‡æ•°ç»„
    target_size: ç›®æ ‡å°ºå¯¸
    
    è¿”å›:
    ç¼©æ”¾åçš„åæ ‡æ•°ç»„
    """
    if len(coords) == 0:
        return coords
        
    x_coords = coords[:, 0]
    y_coords = coords[:, 1]
    
    x_min, x_max = np.min(x_coords), np.max(x_coords)
    y_min, y_max = np.min(y_coords), np.max(y_coords)
    
    # é¿å…é™¤é›¶é”™è¯¯
    x_range = x_max - x_min if x_max > x_min else 1
    y_range = y_max - y_min if y_max > y_min else 1
    
    # ç¼©æ”¾åæ ‡
    scaled_x = ((x_coords - x_min) / x_range) * (target_size - 1)
    scaled_y = ((y_coords - y_min) / y_range) * (target_size - 1)
    
    # å››èˆäº”å…¥ä¸ºæ•´æ•°
    scaled_coords = np.column_stack((np.round(scaled_x).astype(int), 
                                   np.round(scaled_y).astype(int)))
    
    return scaled_coords

def map_to_grid(block_coords: np.ndarray, x_start: int, y_start: int, 
                height: int, width: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    å°†åŒºå—å†…çš„åæ ‡æ˜ å°„åˆ°height*widthçš„ç½‘æ ¼ä¸­
    
    å‚æ•°:
    block_coords: åŒºå—å†…æ‰€æœ‰ç‚¹çš„ç»å¯¹åæ ‡
    x_start: åŒºå—èµ·å§‹xåæ ‡
    y_start: åŒºå—èµ·å§‹yåæ ‡  
    height: ç½‘æ ¼é«˜åº¦
    width: ç½‘æ ¼å®½åº¦
    
    è¿”å›:
    rel_coords: ç›¸å¯¹ç½‘æ ¼åæ ‡(æµ®ç‚¹æ•°æ ¼å¼çš„æ•´æ•°)
    valid_mask: æœ‰æ•ˆç‚¹ä½çš„æ©ç (æœªè¢«ä¸¢å¼ƒçš„ç‚¹)
    """
    # è®¡ç®—ç›¸å¯¹åæ ‡
    rel_coords = block_coords - np.array([x_start, y_start])
    
    # å››èˆäº”å…¥åˆ°æœ€è¿‘çš„ç½‘æ ¼åæ ‡
    grid_coords = np.round(rel_coords).astype(int)
    
    # ç¡®ä¿åæ ‡åœ¨ç½‘æ ¼èŒƒå›´å†…
    grid_coords[:, 0] = np.clip(grid_coords[:, 0], 0, width - 1)
    grid_coords[:, 1] = np.clip(grid_coords[:, 1], 0, height - 1)
    
    # åˆ›å»ºç½‘æ ¼å ç”¨è®°å½•
    grid_occupied = np.zeros((height, width), dtype=bool)
    valid_mask = np.ones(len(rel_coords), dtype=bool)
    
    # æ£€æŸ¥æ¯ä¸ªç‚¹ä½çš„ç½‘æ ¼æ˜¯å¦è¢«å ç”¨
    for i, (x, y) in enumerate(grid_coords):
        if grid_occupied[y, x]:
            # ç½‘æ ¼å·²è¢«å ç”¨ï¼Œä¸¢å¼ƒè¯¥ç‚¹ä½
            valid_mask[i] = False
        else:
            # æ ‡è®°ç½‘æ ¼ä¸ºå·²å ç”¨
            grid_occupied[y, x] = True
            # ä½¿ç”¨æµ®ç‚¹æ•°æ ¼å¼å­˜å‚¨æ•´æ•°åæ ‡
            rel_coords[i] = [float(x), float(y)]
    
    return rel_coords, valid_mask

def create_sample_data(block_coords: np.ndarray, block_expr: np.ndarray, 
                      vocab: np.ndarray, x_start: int, y_start: int,
                      selected_indices: List[int], selected_gene_ids: List[str], 
                      is_selected: List[bool]) -> ad.AnnData:
    """
    åˆ›å»ºæ ·æœ¬æ•°æ®å¹¶è¿”å›AnnDataå¯¹è±¡
    
    å‚æ•°:
    block_coords: åŒºå—åæ ‡
    block_expr: åŒºå—è¡¨è¾¾æ•°æ®
    vocab: åŸºå› è¯æ±‡è¡¨
    x_start: åŒºåŸŸèµ·å§‹Xåæ ‡
    y_start: åŒºåŸŸèµ·å§‹Yåæ ‡
    selected_indices: é€‰ä¸­çš„åŸºå› ç´¢å¼•
    selected_gene_ids: é€‰ä¸­çš„åŸºå› ID
    is_selected: åŸºå› æ˜¯å¦è¢«é€‰ä¸­çš„æ ‡è®°
    
    è¿”å›:
    AnnDataå¯¹è±¡
    """
    # è®¡ç®—ç›¸å¯¹åæ ‡
    rel_coords = block_coords - np.array([x_start, y_start])
    
    # å°†è¡¨è¾¾çŸ©é˜µè½¬æ¢ä¸ºç¨€ç–çŸ©é˜µå¹¶ä¼˜åŒ–æ•°æ®ç±»å‹
    if len(selected_indices) < block_expr.shape[1]:
        expr_data = block_expr[:, selected_indices]
    else:
        expr_data = block_expr
    
    # è½¬æ¢ä¸ºCSRç¨€ç–çŸ©é˜µå¹¶ä¼˜åŒ–æ•°æ®ç±»å‹
    expr_sparse = csr_matrix(expr_data.astype(np.float32))
    
    # åˆ›å»ºAnnDataå¯¹è±¡
    adata = ad.AnnData(
        X=expr_sparse,  # ä½¿ç”¨ç¨€ç–çŸ©é˜µ
        obs={
            'original_index': np.arange(len(block_coords)),
            'x_abs': block_coords[:, 0],
            'y_abs': block_coords[:, 1],
            'x_rel': rel_coords[:, 0],
            'y_rel': rel_coords[:, 1]
        },
        var={
            'gene_ids': selected_gene_ids,
            'is_selected': is_selected
        }
    )
    
    # æ·»åŠ ç©ºé—´ä¿¡æ¯åˆ°obsm
    adata.obsm['coords_map'] = block_coords
    adata.obsm['coords_sample'] = rel_coords
    
    return adata

def process_group(group: h5py.Group, height: int, width: int, depth: int, 
                 min_spot: int, pad_id: int, n_samples_multiplier: int, 
                 output_dir: str, original_coords: np.ndarray) -> Tuple[int, int, int]:
    """
    å¤„ç†å•ä¸ªç»„çš„æ•°æ®
    
    å‚æ•°:
    group: HDF5ç»„å¯¹è±¡
    height: æ ·æœ¬é«˜åº¦
    width: æ ·æœ¬å®½åº¦
    depth: åŸºå› æ·±åº¦
    min_spot: æœ€å°spotæ•°
    pad_id: å¡«å……ID
    n_samples_multiplier: é‡‡æ ·å€æ•°
    output_dir: è¾“å‡ºç›®å½•
    original_coords: åŸå§‹åæ ‡ï¼ˆæœªç¼©æ”¾çš„ï¼‰
    
    è¿”å›:
    (æ ·æœ¬æ•°, è·³è¿‡åŒºå—æ•°, æ·»åŠ spotæ•°)
    """
    # æå–æ•°æ®
    coords = group['coords_map'][:]
    expr = group['expr_map'][:]
    vocab = group['vocab_index'][:]
    
    # å¤„ç†åŸºå› IDç±»å‹
    if isinstance(vocab[0], bytes):
        vocab = np.array([g.decode('utf-8') for g in vocab])
    elif np.issubdtype(vocab.dtype, np.integer):
        vocab = np.array([str(g) for g in vocab])
    target_size = 140
    # ç¼©æ”¾åæ ‡åˆ°140x140èŒƒå›´
    coords = scale_coordinates(coords, target_size)
    
    n_spots, n_genes = expr.shape
    
    # ç¡®å®šç©ºé—´èŒƒå›´ï¼ˆä½¿ç”¨åŸå§‹åæ ‡è®¡ç®—é¢ç§¯ï¼‰
    orig_x_min, orig_x_max = original_coords[:, 0].min(), original_coords[:, 0].max()
    orig_y_min, orig_y_max = original_coords[:, 1].min(), original_coords[:, 1].max()
    
    # è®¡ç®—åŸå§‹æ ·æœ¬é¢ç§¯
    orig_sample_area = (orig_x_max - orig_x_min) * (orig_y_max - orig_y_min)
    sample_area = height * width
    
    # è®¡ç®—é‡‡æ ·æ¬¡æ•°ï¼šæ ·æœ¬é¢ç§¯/sampleé¢ç§¯ * 2
    n_samples = int((target_size * target_size / sample_area) * 2)
    
    # ç¡®å®šç¼©æ”¾åçš„ç©ºé—´èŒƒå›´
    x_min, x_max = coords[:, 0].min(), coords[:, 0].max()
    y_min, y_max = coords[:, 1].min(), coords[:, 1].max()
    
    sample_count = 0
    skipped_blocks = 0
    spots_added = 0
    
    # éšæœºé‡‡æ ·ç©ºé—´åŒºåŸŸ
    for _ in range(n_samples):
        # éšæœºé€‰æ‹©åŒºåŸŸçš„èµ·å§‹åæ ‡
        x_start = random.randint(int(x_min), int(x_max - width))
        y_start = random.randint(int(y_min), int(y_max - height))
        
        x_end = x_start + width
        y_end = y_start + height
        
        # é€‰æ‹©åŒºå—å†…çš„spots
        in_block = (coords[:, 0] >= x_start) & (coords[:, 0] < x_end) & \
                   (coords[:, 1] >= y_start) & (coords[:, 1] < y_end)
        
        # è·³è¿‡spotä¸è¶³çš„åŒºå—
        num_spots = np.sum(in_block)
        if num_spots < min_spot:
            skipped_blocks += 1
            continue
        
        block_coords = coords[in_block]
        block_expr = expr[in_block]
        
        # æ˜ å°„åæ ‡åˆ°ç½‘æ ¼å¹¶å¤„ç†å†²çª
        rel_coords, valid_mask = map_to_grid(
            block_coords, x_start, y_start, height, width
        )
        
        # ä½¿ç”¨æœ‰æ•ˆæ©ç è¿‡æ»¤åæ ‡å’Œè¡¨è¾¾æ•°æ®
        block_coords = block_coords[valid_mask]
        block_expr = block_expr[valid_mask]
        rel_coords = rel_coords[valid_mask]
        
        # æ›´æ–°spotæ•°é‡
        num_spots = np.sum(valid_mask)
        
        # å¦‚æœæœ‰æ•ˆç‚¹ä½æ•°ä¸è¶³ï¼Œè·³è¿‡è¯¥åŒºå—
        if num_spots < min_spot:
            skipped_blocks += 1
            continue
        
        selected_indices = list(range(n_genes))  # é€‰æ‹©æ‰€æœ‰åŸºå› 
        
        # ä¿å­˜æ‰€æœ‰åŸºå› ID
        selected_gene_ids = vocab.tolist()
        
        # åˆ›å»ºæ˜¯å¦è¢«é€‰ä¸­çš„æ ‡è®°
        is_selected = [True] * n_genes
        
        # åˆ›å»ºæ ·æœ¬æ•°æ®
        adata = create_sample_data(
            block_coords, block_expr, vocab, x_start, y_start,
            selected_indices, selected_gene_ids, is_selected
        )
        

        
        # ä¿å­˜ä¸ºH5AD
        output_name = f"{group.name}_sample_{sample_count}.h5ad"
        output_name = output_name.lstrip('/\\')
        output_path = os.path.join(output_dir, output_name)
        adata.write(output_path)
        
        sample_count += 1
        spots_added += num_spots
    
    return sample_count, skipped_blocks, spots_added

def make_samples_h5(input_dir: str, output_dir: str, seed: int = 0, height: int = 14, 
                   width: int = 14, depth: int = 200, min_spot: int = 10, 
                   pad_id: int = 0, n_samples_multiplier: int = 2) -> Dict[str, int]:
    """
    å¤„ç†ç©ºé—´è½¬å½•ç»„æ•°æ®ï¼Œéšæœºé‡‡æ ·ç©ºé—´åŒºåŸŸï¼Œä¿å­˜ä¸ºH5ADæ ¼å¼
    
    å‚æ•°:
    input_dir: åŒ…å«.h5è¾“å…¥æ–‡ä»¶çš„ç›®å½•
    output_dir: è¾“å‡ºæ ·æœ¬çš„ç›®å½•
    seed: éšæœºç§å­(é»˜è®¤0)
    height: æ¯ä¸ªæ ·æœ¬çš„yè½´é«˜åº¦(é»˜è®¤14)
    width: æ¯ä¸ªæ ·æœ¬çš„xè½´å®½åº¦(é»˜è®¤14)
    depth: ä½¿ç”¨çš„åŸºå› æ•°é‡(é»˜è®¤200)
    min_spot: æ ·æœ¬åŒ…å«çš„æœ€å°spotæ•°(é»˜è®¤10)
    pad_id: ç”¨äºå¡«å……ä¸è¶³åŸºå› çš„æ ‡è¯†ç¬¦(é»˜è®¤0)
    n_samples_multiplier: é‡‡æ ·æ¬¡æ•°å€æ•°(é»˜è®¤2)
    
    è¿”å›:
    å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    # è®¾ç½®éšæœºç§å­
    np.random.seed(seed)
    random.seed(seed)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰h5æ–‡ä»¶
    h5_files = [f for f in os.listdir(input_dir) if f.endswith('.h5')]
    if not h5_files:
        print("âš ï¸ è­¦å‘Š: è¾“å…¥ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°.h5æ–‡ä»¶")
        return {}
    
    print(f"ğŸ” æ‰¾åˆ° {len(h5_files)} ä¸ª.h5æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
    print(f"âš™ï¸ é…ç½®å‚æ•°: height={height}, width={width}, depth={depth}, min_spot={min_spot}")
    print(f"ğŸ¯ é‡‡æ ·ç­–ç•¥: éšæœºé‡‡æ ·{height}x{width}åŒºåŸŸ")
    print(f"ğŸ’¾ è¾“å‡ºæ ¼å¼: H5AD (AnnDataæ ¼å¼)")
    
    # ç»Ÿè®¡å˜é‡
    total_samples = 0
    skipped_files = 0
    skipped_groups = 0
    skipped_blocks = 0
    total_spots = 0
    
    # è¿›åº¦æ¡ï¼šæ–‡ä»¶å¤„ç†
    file_pbar = tqdm(h5_files, desc="æ–‡ä»¶å¤„ç†")
    
    for file_name in file_pbar:
        file_pbar.set_postfix(file=file_name)
        file_path = os.path.join(input_dir, file_name)
        
        try:
            with h5py.File(file_path, 'r') as h5_file:
                groups = list(h5_file.keys())
                if not groups:
                    skipped_files += 1
                    print(f"âš ï¸ è·³è¿‡: æ–‡ä»¶ {file_name} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»„")
                    continue
                
                # ç»„å¤„ç†è¿›åº¦
                group_pbar = tqdm(groups, desc=f"æ ·æœ¬å¤„ç†", leave=False)
                
                for group_name in group_pbar:
                    group_pbar.set_postfix(group=group_name)
                    
                    try:
                        group = h5_file[group_name]
                        
                        # è·å–åŸå§‹åæ ‡ï¼ˆæœªç¼©æ”¾çš„ï¼‰
                        original_coords = group['coords_map'][:]
                        
                        samples, skipped, spots = process_group(
                            group, height, width, depth, min_spot, 
                            pad_id, n_samples_multiplier, output_dir, original_coords
                        )
                        
                        total_samples += samples
                        skipped_blocks += skipped
                        total_spots += spots
                        
                        group_pbar.set_postfix(spots=f"{spots} spots")
                    
                    except Exception as e:
                        skipped_groups += 1
                        print(f"âš ï¸ è·³è¿‡ç»„ {group_name}: {str(e)}")
                
                group_pbar.close()
        
        except Exception as e:
            skipped_files += 1
            print(f"ğŸš¨ é”™è¯¯å¤„ç†æ–‡ä»¶ {file_name}: {str(e)}")
    
    # å¤„ç†å®Œæˆåçš„ç»Ÿè®¡ä¿¡æ¯
    print(f"\nâœ… å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"  åˆ›å»ºæ ·æœ¬: {total_samples}ä¸ª")
    print(f"  é‡‡æ ·å°è¯•: {total_samples + skipped_blocks}æ¬¡")
    print(f"  è·³è¿‡æ–‡ä»¶: {skipped_files}ä¸ª")
    print(f"  è·³è¿‡æ ·æœ¬ç»„: {skipped_groups}ä¸ª")
    print(f"  è·³è¿‡åŒºå—: {skipped_blocks}ä¸ª (spot<{min_spot})")
    print(f"  æ€»å¤„ç†spots: {total_spots}ä¸ª")
    
    if total_samples == 0:
        print("âŒ æœªåˆ›å»ºä»»ä½•æ ·æœ¬ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®å’Œå‚æ•°é…ç½®")
    else:
        print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {output_dir} (H5ADæ ¼å¼)")
        print(f"ğŸ”¬ æ¯ä¸ªæ ·æœ¬åŒ…å«æ‰€æœ‰åŸºå›  (ä½¿ç”¨ç¨€ç–çŸ©é˜µå­˜å‚¨)")
    
    return {
        'total_samples': total_samples,
        'skipped_files': skipped_files,
        'skipped_groups': skipped_groups,
        'skipped_blocks': skipped_blocks,
        'total_spots': total_spots
    }

def main(input_dir,vocab_path,output_dir,hv_genes_path,seed=0,height=14,width=14,depth=200,min_spot=10):
    if os.path.exists(vocab_path):
        print(f"ä» {vocab_path} åŠ è½½åŸºå› ç´¢å¼•...")
        with open(vocab_path, 'r') as f:
            vocab = json.load(f)
    else:
        vocab = None
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°è¯æ±‡è¡¨æ–‡ä»¶ {vocab_path}")
    #hv_genes_id = find_hv_genes(input_dir,depth,hv_genes_path)
    make_samples_h5(input_dir,output_dir,seed,height,width,depth,min_spot,vocab["<pad>"])

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="make sample with size of 14*14*200"
    )
    parser.add_argument('-i', '--input-dir',      default='project1/spatial_data/all_data',    help='Directory of raw inputs')
    parser.add_argument('-o', '--output-dir',     default='project1/spatial_data/samples16',     help='Base output directory')
    parser.add_argument('-v', '--vocab-path',     default='project1/spatial_data/spatial_data/new_vocab.json',               help='Path to vocab JSON file')
    parser.add_argument('-hv', '--hv-genes-path', default='project1/spatial_data/spatial_data/samples/hv_genes_id.json',     help='Path to hv genes vocab JSON file')    
    parser.add_argument('-min', '--min-spot', type=int, default=57,              help='min spot of each sample (height*width*0.1)')
    parser.add_argument('-s', '--seed',       type=int, default=0,              help='Random seed')
    parser.add_argument('-H', '--height',     type=int, default=16,             help='height of sample')
    parser.add_argument('-W', '--width',      type=int, default=16,             help='width of sample')
    parser.add_argument('-D', '--depth',      type=int, default=512,            help='depth of sample')
    args = parser.parse_args()
    main(args.input_dir, args.vocab_path, args.output_dir, args.hv_genes_path, args.seed,args.height,args.width,args.depth,args.min_spot)


    